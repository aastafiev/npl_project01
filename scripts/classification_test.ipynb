{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dfs(gen_file_path, in_file_path, feature):\n",
    "    print \"Load to DataFrame gen_file_path\"\n",
    "    gen_df = pd.read_csv(gen_file_path,\n",
    "                         encoding='utf-8',\n",
    "                         sep='\\t',\n",
    "                         skipinitialspace=True,\n",
    "                         usecols=['gender', 'age', 'uid']\n",
    "                         )\n",
    "    gen_train_df = gen_df[~((gen_df['gender'] == '-') & (gen_df['age'] == '-'))]\n",
    "    gen_test_df = gen_df[(gen_df['gender'] == '-') & (gen_df['age'] == '-')]\n",
    "    gen_test_df_uids = gen_test_df['uid'].unique()\n",
    "\n",
    "    print \"Load to DataFrame meta data\"\n",
    "    meta_df = pd.read_csv(in_file_path,\n",
    "                          encoding='utf-8',\n",
    "                          sep='\\t',\n",
    "                          skipinitialspace=True,\n",
    "                          )\n",
    "\n",
    "    print \"Preparing Train DataFrame\"\n",
    "    meta_train_df = pd.merge(meta_df, gen_train_df, on='uid', sort=False)\n",
    "    meta_train_series = meta_train_df.groupby(feature)['meta']\\\n",
    "        .apply(lambda x: u' '.join([unicode(ss).replace('&nbsp', ' ').replace('&quot', '\"')\n",
    "                                   .replace('&laquo', '\"').replace('&raquo', '\"') for ss in x.tolist()]))\n",
    "    meta_train_df = pd.DataFrame(meta_train_series, index=meta_train_series.index, columns=['meta'])\n",
    "\n",
    "    print \"Preparing Test DataFrame\"\n",
    "    meta_test_df = meta_df[meta_df['uid'].isin(gen_test_df_uids.tolist())]\n",
    "    meta_test_series = meta_test_df.groupby('uid')['meta']\\\n",
    "        .apply(lambda x: u' '.join([unicode(ss).replace('&nbsp', ' ').replace('&quot', '\"')\n",
    "                                   .replace('&laquo', '\"').replace('&raquo', '\"') for ss in x.tolist()]))\n",
    "    meta_test_df = pd.DataFrame(meta_test_series, index=meta_test_series.index, columns=['meta'])\n",
    "\n",
    "    return gen_train_df, gen_test_df, meta_train_df, meta_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokenz = TfidfVectorizer().build_tokenizer()(text)\n",
    "        lemmas = []\n",
    "        for t in tokenz:\n",
    "            if len(t) > 2:\n",
    "                p = self.morph.parse(t)\n",
    "                if 'LATN' in p[0].tag: # and re.search('!\\d+', p[0].normal_form)\n",
    "                    lemmas.append(self.wnl.lemmatize(t))\n",
    "                elif 'NUMB' in p[0].tag:\n",
    "                    continue\n",
    "                elif 'UNKN' in p[0].tag:\n",
    "                    continue\n",
    "                elif 'ROMN' in p[0].tag:\n",
    "                    continue\n",
    "                else:\n",
    "                    lemmas.append(p[0].normal_form)\n",
    "        return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\nCooking data for feature 'age'\nLoad to DataFrame gen_file_path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load to DataFrame meta data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Train DataFrame\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Test DataFrame\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking data time: 15.870s\n________________________________________________________________________________\nVectorizing\nTrain\n"
     ]
    }
   ],
   "source": [
    "feature = 'age'\n",
    "print '_' * 80\n",
    "print \"Cooking data for feature '%s'\" % feature\n",
    "t0 = time()\n",
    "\n",
    "gen_file_path = '/Users/usual/PycharmProjects/npl_project01/data/gender_age_dataset.txt'\n",
    "in_file_path = '/Users/usual/PycharmProjects/npl_project01/data/csv/uid_meta_fixed.csv'\n",
    "\n",
    "gen_train_df, gen_test_df, meta_train_df, meta_test_df = prepare_dfs(gen_file_path, in_file_path, feature)\n",
    "\n",
    "command_time = time() - t0\n",
    "print \"cooking data time: %0.3fs\" % command_time\n",
    "# Print out prepared data info\n",
    "#     print u'len(gen_train_df) = %d\\nlen(gen_test_df) = %d\\nlen(meta_train_df) = %d\\n' \\\n",
    "#           u'len(meta_test_df) = %d\\nlen(tmeta_train_df) = %d\\nlen(tmeta_test_df) = %d' % \\\n",
    "#           (len(gen_train_df),\n",
    "#            len(gen_test_df),\n",
    "#            len(meta_train_df),\n",
    "#            len(meta_test_df),\n",
    "#            len(tmeta_train_df),\n",
    "#            len(tmeta_test_df))\n",
    "\n",
    "# Vectorizing\n",
    "print '_' * 80\n",
    "print \"Vectorizing\"\n",
    "print \"Train\"\n",
    "t0 = time()\n",
    "\n",
    "# uids_train = tmeta_train_df['uid'].tolist()\n",
    "y_train = meta_train_df.index.values\n",
    "corpus_train = meta_train_df['meta'].tolist()\n",
    "stop_words = stopwords.words('english') + stopwords.words('russian') + ['ru', 'com']\n",
    "vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words=stop_words)  # ngram_range=(1, 2)\n",
    "x_train = vectorizer.fit_transform(corpus_train)\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "command_time = time() - t0\n",
    "print \"vectorizing train time: %0.3fs\" % command_time\n",
    "\n",
    "# print x_train.shape\n",
    "# print x_train.toarray()\n",
    "#\n",
    "# ch2 = SelectKBest(chi2, k=10)\n",
    "# x_train = ch2.fit_transform(x_train, y_train)\n",
    "\n",
    "print \"Test\"\n",
    "t0 = time()\n",
    "# uids_test = tmeta_test_df['uid'].tolist()\n",
    "y_test = meta_test_df.index.values\n",
    "corpus_test = meta_test_df['meta'].tolist()\n",
    "x_test = vectorizer.transform(corpus_test)\n",
    "\n",
    "command_time = time() - t0\n",
    "print \"vectorizing test time: %0.3fs\" % command_time\n",
    "# x_test = ch2.transform(x_test)\n",
    "\n",
    "# Print out x_train\n",
    "print x_train.shape\n",
    "print x_train.toarray()\n",
    "\n",
    "print x_test.shape\n",
    "print x_test.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}