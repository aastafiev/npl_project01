Алексей Астафьев 26.03.2017 02:07


Доработал модуль find_key.py

Входной основной файл с данными gender_age_dataset.txt.
settings.py - глобальные натройки.

В функции parse_to_files() генерится три файла (разделители - табуляция, без заголовков):
1. uid_meta.txt
    Структура (uid, metadata), где uid из основного файла, metadata - то что удалось по каждой ссылке, по которым ходил пользователь, вытащить из тэгов meta с аттрибутом name из
    META_NAMES = ['description',
              'og:description',
              'twitter:description'
              'keywords',
              'title',
              'og:title',
              'twitter:title']


TODO: Это надо рабить на слова и уже положить в отдельный файл для будущего тематического моделирования и алгоритмов ML. Стоит очисить от всяких предлогов и учесть как минимум два языка: english, russian. Ну и проверить/улучшить код.

2. uid_lost_url.txt
    Структура (uid, url, error)
    Здесть все адреса, у которых не удалось прочитать метаинформациюЗдесть все адреса, у которых не удалось прочитать метаинформацию.
    Возможны два вида ошибок: bad_url, empty_meta
        bad_url - битый url. даже в браузере не открывается.
        empty_data - не удалось вытащить meta. либо ее действительно нет, либо какая-то ошибка в теге

TODO: Яна, ты вызвалась проанализировать в SQL. Мне кажется имеет смысл рассмотреть этот в следующих направлениях:
    Много ли таких адресов по отношению к общему количеству адресов (файл uid_domain_url_timestamp.txt)?
    Много ли безвозвратных?
    Может ты еще что-то придумаешь?

Да и вообще можно все файлы грузануть в SQL :) и посмотреть что вообще получается.

P.S. Кстате, некторые адреса могли не открыться из-за моего ddos с моего хоста. Есть у кого идею как обйти это? Можно попробовать сделать sleep между вызовами, либо повторяющиеся адреса позже как-то рапускать, но четкого алгоритма пока не вырисовывается. Давайте поштормим эту тему?


3. uid_domain_url_timestamp.txt
    Структура (uid, domain, url, timestamp)
    По сути разобранный json в основном фале.

TODO: Давайте поштормим на предмет идей как объединить домены/адреса.


Оработку исходных данных основывал на ссылке, присланной Владимиром, а именно:

- Можно проанализировать сам домен и сгруппировать сайты по нему;

есть затравка здесь uid_domain_url_timestamp.txt
стоит сделать дополнительную обработку

- Можно скачать описательную часть страницы (title, keywords, метаописания) и проанализировать их;

Здесь собрал в один файл. См. п.1

- Самый тяжёлый, но глубокий вариант – это перейти по ссылке и скачать весь код страницы и постараться определить тематику страницы;

Если выясниться, что слишком много записей в uid_lost_url.txt, то можно попробовать

- Ещё один подход, это связать посещения пользователем сайтов в цепочку и определить логику в последовательности переходов;
Есть затравка в uid_domain_url_timestamp.txt
Надо штормить

- Отдельной «фичей» могут стать мобильные домены (m.facebook.com), они тоже могут быть полезны для определения категории пользователя.
Надо штормить


Запуск:

seq -w 10000 1000 40000 | xargs -n1 -P8 -I SKIPROWS python find_keys.py \
    --timeout=60 \
    --skiprows=SKIPROWS \
    --nrows=1000 \
    --meta-file-path=csv/uid_meta_SKIPROWS_1000.csv \
    --lost-urls-file-path=csv/uid_lost_url_SKIPROWS_1000.csv \
    --domain-urls-timestamp-file-path=csv/uid_domain_url_timestamp_SKIPROWS_1000.csv \
    --log-file=log/find_keys_SKIPROWS_1000.log

